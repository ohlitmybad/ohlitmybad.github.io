# .github/workflows/matrix-scraping.yml
name: MATRIX TURBO Player Data Scraping (15 IPs)
on:
  schedule:
    # Run on the 1st of every month at 2 AM UTC
    - cron: '0 2 1 * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  # MATRIX STRATEGY - 15 machines = 15 different IPs = 1 per search URL!
  matrix-scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hours per worker
    strategy:
      matrix:
        worker_id: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  # 15 workers!
      fail-fast: true  # Don't cancel other workers if one fails
      max-parallel: 15  # All 15 workers run simultaneously
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests pandas beautifulsoup4 openpyxl lxml
        echo "Dependencies installed for worker ${{ matrix.worker_id }}"
    
    - name: Run Matrix Worker (1 URL per worker)
      run: |
        echo "ðŸš€ Starting Matrix Worker ${{ matrix.worker_id }}/15 (1 dedicated search URL)..."
        python scraper.py --worker-id ${{ matrix.worker_id }} --total-workers 15
        echo "âœ… Worker ${{ matrix.worker_id }} completed its dedicated search"
    
    - name: Upload worker results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: worker-${{ matrix.worker_id }}-results
        path: |
          transfermarkt_worker_${{ matrix.worker_id }}_*.xlsx
          scraper.log
        retention-days: 30

  
  # MERGE JOB - Combines all 15 worker results from XLSX files
  merge-results:
    needs: matrix-scrape
    runs-on: ubuntu-latest
    if: always()

    permissions:
      contents: write

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install merge dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas openpyxl

    - name: Download all worker results
      uses: actions/download-artifact@v4
      with:
        path: worker-results/

    - name: Display downloaded file structure
      run: ls -R worker-results/

    - name: Create merge script
      run: |
        cat > merge_xlsx.py << 'EOF'
        import pandas as pd
        import os
        from glob import glob
        import sys

        def merge_xlsx_files():
            print('ðŸ”„ Merging 15 worker XLSX results...')
            
            # Find all XLSX files from workers
            xlsx_files = glob('worker-results/worker-*-results/transfermarkt_worker_*.xlsx')
            
            if not xlsx_files:
                print('âŒ No XLSX result files found to merge.')
                return False, 0, 0
            
            print(f'Found {len(xlsx_files)} XLSX files to merge')
            
            all_data = []
            
            for xlsx_file in xlsx_files:
                print(f'Processing: {xlsx_file}')
                try:
                    df = pd.read_excel(xlsx_file)
                    all_data.append(df)
                    print(f'Added {len(df)} players from {xlsx_file}')
                except Exception as e:
                    print(f'Error reading {xlsx_file}: {e}')
                    continue
            
            if not all_data:
                print('âŒ No valid data found in XLSX files.')
                return False, 0, 0
            
            # Combine all dataframes
            combined_df = pd.concat(all_data, ignore_index=True)
            print(f'Combined total: {len(combined_df)} players')
            
            # Remove duplicates based on Transfermarkt URL (assuming it's a column)
            # Check what columns are available
            print(f'Available columns: {list(combined_df.columns)}')
            
            # Remove duplicates - adjust column name based on your actual data
            if 'Transfermarkt' in combined_df.columns:
                unique_df = combined_df.drop_duplicates(subset=['Transfermarkt'])
                print(f'After removing duplicates by Transfermarkt URL: {len(unique_df)} players')
            elif 'transfermarkt_url' in combined_df.columns:
                unique_df = combined_df.drop_duplicates(subset=['transfermarkt_url'])
                print(f'After removing duplicates by transfermarkt_url: {len(unique_df)} players')
            else:
                # Fallback: remove duplicates by name if Transfermarkt column not found
                if 'Name' in combined_df.columns:
                    unique_df = combined_df.drop_duplicates(subset=['Name'])
                    print(f'After removing duplicates by Name: {len(unique_df)} players')
                else:
                    unique_df = combined_df
                    print('No suitable column for deduplication found, keeping all records')
            
            # Sort by value if possible
            if 'Value (Mâ‚¬)' in unique_df.columns:
                unique_df = unique_df.sort_values(by='Value (Mâ‚¬)', ascending=False)
            elif 'Value' in unique_df.columns:
                unique_df = unique_df.sort_values(by='Value', ascending=False)
            
            # Save merged file
            output_filename = 'jose_new.xlsx'
            with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:
                unique_df.to_excel(writer, index=False, sheet_name='Players')
                worksheet = writer.sheets['Players']
                # Auto-adjust column widths
                for col in worksheet.columns:
                    max_length = 0
                    column = col[0].column_letter
                    for cell in col:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column].width = adjusted_width
            
            total_players = len(unique_df)
            
            # Count Instagram links if column exists
            instagram_count = 0
            if 'Instagram' in unique_df.columns:
                instagram_count = unique_df['Instagram'].notna().sum()
            elif 'instagram' in unique_df.columns:
                instagram_count = unique_df['instagram'].notna().sum()
            
            print(f'âœ… Merged file created: {output_filename}')
            print(f'ðŸ‘¥ Total unique players: {total_players}')
            print(f'ðŸ“¸ With Instagram: {instagram_count}')
            print(f'ðŸŒ Data collected from 15 different IPs')
            
            return True, total_players, instagram_count

        if __name__ == "__main__":
            success, player_count, instagram_count = merge_xlsx_files()
            
            # Write outputs for GitHub Actions
            with open('merge_results.txt', 'w') as f:
                f.write(f"MERGE_SUCCESS={str(success).lower()}\n")
                f.write(f"PLAYER_COUNT={player_count}\n")
                f.write(f"INSTAGRAM_COUNT={instagram_count}\n")
            
            if not success:
                sys.exit(1)
        EOF

    - name: Run merge script
      id: merge_step
      run: |
        python merge_xlsx.py
        
        # Read the results
        if [ -f merge_results.txt ]; then
          while IFS= read -r line; do
            echo "$line" >> $GITHUB_OUTPUT
          done < merge_results.txt
        else
          echo "MERGE_SUCCESS=false" >> $GITHUB_OUTPUT
          echo "PLAYER_COUNT=0" >> $GITHUB_OUTPUT
          echo "INSTAGRAM_COUNT=0" >> $GITHUB_OUTPUT
        fi

    - name: Commit and push merged results
      if: steps.merge_step.outputs.MERGE_SUCCESS == 'true'
      run: |
        git config user.name "Matrix Scraper Bot"
        git config user.email "action@github.com"

        git add jose_new.xlsx

        if git diff --staged --quiet; then
          echo "â„¹ï¸ No changes to commit."
        else
          PLAYER_COUNT="${{ steps.merge_step.outputs.PLAYER_COUNT }}"
          INSTAGRAM_COUNT="${{ steps.merge_step.outputs.INSTAGRAM_COUNT }}"
          COMMIT_MSG="ðŸš€ 15-IP MATRIX Update: ${PLAYER_COUNT} players (${INSTAGRAM_COUNT} with Instagram) - $(date +'%Y-%m-%d %H:%M UTC')"

          git commit -m "$COMMIT_MSG"
          git push
          echo "âœ… Matrix results from 15 IPs pushed successfully."
        fi

    - name: Upload final merged results
      uses: actions/upload-artifact@v4
      if: always() && steps.merge_step.outputs.MERGE_SUCCESS == 'true'
      with:
        name: final-merged-results-15-workers
        path: jose_new.xlsx
        retention-days: 90

    - name: Matrix Summary
      if: always()
      run: |
        echo "## ðŸŽ‰ 15-IP MATRIX SCRAPING COMPLETE!" >> $GITHUB_STEP_SUMMARY
        if [ "${{ steps.merge_step.outputs.MERGE_SUCCESS }}" == "true" ]; then
          echo "- **Status:** âœ… Successful 15-Worker Merge & Push" >> $GITHUB_STEP_SUMMARY
          echo "- **IPs used:** 15 different GitHub runner IPs" >> $GITHUB_STEP_SUMMARY
          echo "- **Efficiency:** 1 search URL per IP (perfect distribution)" >> $GITHUB_STEP_SUMMARY
          echo "- **Final file:** jose_new.xlsx" >> $GITHUB_STEP_SUMMARY
          echo "- **Total players:** ${{ steps.merge_step.outputs.PLAYER_COUNT }}" >> $GITHUB_STEP_SUMMARY
          echo "- **With Instagram:** ${{ steps.merge_step.outputs.INSTAGRAM_COUNT }}" >> $GITHUB_STEP_SUMMARY
        else
          echo "- **Status:** âŒ Merge Failed" >> $GITHUB_STEP_SUMMARY
          echo "- **Details:** Check logs for merge errors" >> $GITHUB_STEP_SUMMARY
        fi
